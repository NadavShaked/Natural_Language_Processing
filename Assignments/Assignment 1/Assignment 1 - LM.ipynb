{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
    "\n",
    "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xC-87z2GWMq",
    "outputId": "5dd5f450-f9b4-41dc-a04c-301d9061c0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nlp-course' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kfirbar/nlp-course.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.csv     es.json    in.csv     it.json    pt.csv     test.json  tl.csv\r\n",
      "en.json    fr.csv     in.json    nl.csv     pt.json    tests.csv  tl.json\r\n",
      "es.csv     fr.json    it.csv     nl.json    test.csv   tests.json\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls nlp-course/lm-languages-data-new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xCfzsITW8Yaj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def preprocess():\n",
    "    folder_path = \"./nlp-course/lm-languages-data-new\"\n",
    "    vocabulary = []\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file[-4:] == \"json\":\n",
    "            path_json = os.path.join(folder_path, file)\n",
    "            with open(path_json, \"r\",  encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "            for tweet_num, tweet_text in json_data['tweet_text'].items():\n",
    "                tweet_text = tweet_text\n",
    "                for character in tweet_text:\n",
    "                    if character not in vocabulary:\n",
    "                        vocabulary.append(character)\n",
    "\n",
    "    vocabulary.append(\"<s>\")\n",
    "    vocabulary.append(\"<e>\")\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function `lm` that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kMC_u8eQbVvZ"
   },
   "outputs": [],
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
    "    # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
    "    # data_file_path - the data_file from which we record probabilities for our model\n",
    "    # add_one - True/False (use add_one smoothing or not)\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    dict = {}\n",
    "\n",
    "    with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    for tweet_num, tweet_text in json_data['tweet_text'].items():\n",
    "        tweet_len = len(tweet_text)\n",
    "\n",
    "        for j in range(tweet_len):\n",
    "            i = max(0, j - n + 2)\n",
    "            sequence = tweet_text[i: j + 1]\n",
    "            if i == 0:\n",
    "                sequence = \"<s>\" * (n - 2 - j) + sequence\n",
    "\n",
    "            if j == tweet_len - 1:\n",
    "                next_char_of_sequence = \"<e>\"\n",
    "            else:\n",
    "                next_char_of_sequence = tweet_text[j + 1]\n",
    "\n",
    "            if sequence in dict:\n",
    "                dict[sequence][\"count\"] += 1\n",
    "                next_char_dict = dict[sequence][\"next_char_dict\"]\n",
    "                if next_char_of_sequence in next_char_dict:\n",
    "                    next_char_dict[next_char_of_sequence] += 1\n",
    "                else:\n",
    "                    next_char_dict[next_char_of_sequence] = 1\n",
    "            else:\n",
    "                dict[sequence] = {\n",
    "                    \"next_char_dict\": {\n",
    "                        next_char_of_sequence: 1\n",
    "                    },\n",
    "                    \"count\": 1\n",
    "                }\n",
    "\n",
    "    probabilities_dict = {}\n",
    "\n",
    "    for sequence, sequence_data in dict.items():\n",
    "        sequence_count = sequence_data[\"count\"]\n",
    "        probabilities_dict[sequence] = {}\n",
    "\n",
    "        if add_one:\n",
    "            for next_char in vocabulary:\n",
    "                if next_char in sequence_data[\"next_char_dict\"]:\n",
    "                    prob = (sequence_data[\"next_char_dict\"][next_char] + 1) / (sequence_count + vocabulary_size)\n",
    "                    probabilities_dict[sequence][next_char] = prob\n",
    "            probabilities_dict[sequence][\"not_exist\"] = 1 / (sequence_count + vocabulary_size)\n",
    "        else:\n",
    "            for next_char, next_char_count in sequence_data[\"next_char_dict\"].items():\n",
    "                prob = next_char_count / sequence_count\n",
    "                probabilities_dict[sequence][next_char] = prob\n",
    "\n",
    "    if add_one:\n",
    "        probabilities_dict[\"not_exist\"] = 1 / vocabulary_size\n",
    "    else:\n",
    "        probabilities_dict[\"not_exist\"] = 10 ** -20\n",
    "\n",
    "    return probabilities_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_perplexity(tweet_text, n, model):\n",
    "    entropy_tweet = 0\n",
    "    tweet_len = len(tweet_text)\n",
    "\n",
    "    for j in range(tweet_len):\n",
    "        i = max(0, j - n + 2)\n",
    "        sequence = tweet_text[i: j + 1]\n",
    "        if i == 0:\n",
    "            sequence = \"<s>\" * (n - 2 - j) + sequence\n",
    "        if j == tweet_len - 1:\n",
    "            next_char_of_sequence = \"<e>\"\n",
    "        else:\n",
    "            next_char_of_sequence = tweet_text[j + 1]\n",
    "        if sequence not in model:\n",
    "            prob = model[\"not_exist\"]\n",
    "        else:\n",
    "            if next_char_of_sequence in model[sequence]:\n",
    "                prob = model[sequence][next_char_of_sequence]\n",
    "            else:\n",
    "                if \"not_exist\" in model[sequence]:\n",
    "                    prob = model[sequence][\"not_exist\"]\n",
    "                else:\n",
    "                    prob = model[\"not_exist\"]\n",
    "\n",
    "        entropy_tweet += np.log(prob)\n",
    "\n",
    "    entropy_tweet = -(1 / tweet_len) * entropy_tweet\n",
    "    return 2 ** entropy_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F0kkMn328-lJ"
   },
   "outputs": [],
   "source": [
    "def eval(n, model, data_file):\n",
    "    # n - the n-gram that you used to build your model (must be the same number)\n",
    "    # model - the dictionary (model) to use for calculating perplexity\n",
    "    # data_file - the tweets file that you wish to claculate a perplexity score for\n",
    "\n",
    "    perplexity = 1\n",
    "    with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    num_tweets = len(json_data['tweet_text'])\n",
    "    for tweet_num, tweet_text in json_data['tweet_text'].items():\n",
    "        perplexity_tweet = tweet_perplexity(tweet_text, n, model)\n",
    "        perplexity += perplexity_tweet\n",
    "\n",
    "    perplexity = (1 / num_tweets) * perplexity\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
    "\n",
    "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_and_models(n, vocabulary, add_one):\n",
    "    folder_path = \"./nlp-course/lm-languages-data-new\"\n",
    "    models = {}\n",
    "    files = {}\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file[-4:] == \"json\":\n",
    "            language_name = file.split(\".\")[0]\n",
    "            if language_name not in [\"en\", \"es\", \"fr\", \"in\", \"it\", \"nl\", \"pt\", \"tl\"]:\n",
    "                continue\n",
    "            path_json = os.path.join(folder_path, file)\n",
    "            model = lm(n, vocabulary, path_json, add_one)\n",
    "            files[language_name] = path_json\n",
    "            models[language_name] = model\n",
    "\n",
    "    return models, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "caAxLE9s_fvn"
   },
   "outputs": [],
   "source": [
    "def match(n, add_one, name_csv=\"part4\"):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "\n",
    "    vocabulary = preprocess()\n",
    "    languages = []\n",
    "    models, files = get_files_and_models(n, vocabulary, add_one)\n",
    "    perplexity_df = pd.DataFrame(index=languages, columns=languages)\n",
    "\n",
    "    for lang1, model in models.items():\n",
    "        for lang2, file in files.items():\n",
    "            perplexity = eval(n, model, file)\n",
    "            perplexity_df.loc[lang1, lang2] = perplexity\n",
    "\n",
    "    perplexity_df.to_csv(f\"312494923_316550797_{name_csv}.csv\")\n",
    "    return perplexity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "match(3, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
    "\n",
    "Load each result to a dataframe and save to a CSV with the name format: \n",
    "\n",
    "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "\n",
    "For cases without add_one:\n",
    "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "Follow the same format for n2,n3, and n4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nk32naXyAMdl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n - 1, add_one - True\n",
      "           nl         pt         en         it         tl         in  \\\n",
      "nl  13.055351  14.048366  13.801164  13.725641  15.456463  14.441502   \n",
      "pt  14.429282  12.532321  14.132458  13.645197  15.282229  14.702816   \n",
      "en  13.737921  14.076758  12.988100  13.560570  14.773840  14.368623   \n",
      "it  14.173893  13.697841  13.867212  12.937941  15.522299  14.813791   \n",
      "tl  14.056637  14.329370  13.876513  13.868441  13.641067  13.659617   \n",
      "in  14.301263  14.584162  14.068862  14.205196  14.340620  13.121042   \n",
      "fr  14.457423  13.816761  13.982238  13.620349  16.344474  15.124689   \n",
      "es  14.476219  13.432943  14.037528  13.583052  15.823381  14.899855   \n",
      "\n",
      "           fr         es  \n",
      "nl  13.551220  13.579234  \n",
      "pt  13.299141  12.793658  \n",
      "en  13.672072  13.551434  \n",
      "it  13.240264  13.113641  \n",
      "tl  14.605160  13.832167  \n",
      "in  14.598011  14.032241  \n",
      "fr  12.635822  13.426735  \n",
      "es  13.375765  12.403975  \n",
      "n - 1, add_one - False\n",
      "              nl             pt             en           it            tl  \\\n",
      "nl  1.305935e+01  318771.320356  401583.574278    23.936323  1.326240e+08   \n",
      "pt  8.939317e+06      12.528756  316546.884426  3809.395369  4.688357e+05   \n",
      "en  1.117592e+02     421.858692      12.985541    17.506326  3.965442e+04   \n",
      "it  3.855309e+03  319541.169267  297790.710859    12.931398  1.318729e+08   \n",
      "tl  8.457086e+01  229114.535858  161933.093806    26.581135  1.373913e+01   \n",
      "in  3.347819e+03  224451.225576    9005.418681    43.432924  5.976710e+02   \n",
      "fr  8.785785e+06  316870.563482  284901.786746   514.874698  1.336546e+08   \n",
      "es  7.660330e+04  229078.576237  238445.245946    32.553535  1.322559e+08   \n",
      "\n",
      "              in          fr            es  \n",
      "nl  2.055671e+06   99.798404  6.454901e+05  \n",
      "pt  9.508267e+05  168.455119  2.237443e+02  \n",
      "en  2.020491e+06  165.867866  9.345123e+05  \n",
      "it  1.653107e+06  132.935230  9.205213e+05  \n",
      "tl  6.824800e+04   28.776710  1.065930e+06  \n",
      "in  1.315183e+01   24.500842  4.555968e+01  \n",
      "fr  5.690200e+05   12.634479  7.023423e+06  \n",
      "es  9.381141e+05  149.162499  1.240336e+01  \n",
      "n - 2, add_one - True\n",
      "           nl         pt         en         it         tl         in  \\\n",
      "nl   8.877778  12.290810  10.500405  11.438371  11.783698  11.651019   \n",
      "pt  12.706361   8.402139  12.091189  10.374597  12.789091  13.101194   \n",
      "en  10.756928  11.775901   8.753559  10.760406  11.264858  11.452803   \n",
      "it  12.286572  10.614571  11.646922   8.470858  12.185961  12.591507   \n",
      "tl  11.525195  11.838351  10.514426  10.669552   8.985507  10.370755   \n",
      "in  11.266195  12.314908  11.070991  11.070268  10.413511   9.091610   \n",
      "fr  11.420990  10.953503  10.832107  10.506754  12.675783  12.330718   \n",
      "es  12.323374  10.135697  11.763560  10.047446  12.504473  12.704566   \n",
      "\n",
      "           fr         es  \n",
      "nl  11.319826  11.449697  \n",
      "pt  11.113838   9.475607  \n",
      "en  11.020415  10.686038  \n",
      "it  11.073619   9.772669  \n",
      "tl  12.176579  10.764721  \n",
      "in  12.283293  11.286427  \n",
      "fr   8.424735  10.096080  \n",
      "es  10.993153   8.128654  \n",
      "n - 2, add_one - False\n",
      "              nl            pt            en            it            tl  \\\n",
      "nl  7.825103e+00  4.698805e+07  2.613325e+06    579.990553  8.106381e+09   \n",
      "pt  1.921113e+09  7.221651e+00  4.042616e+05  88637.872317  1.358459e+08   \n",
      "en  3.631127e+06  3.903308e+06  7.753358e+00    541.914175  1.788066e+07   \n",
      "it  2.946115e+07  4.557538e+07  1.757246e+06      7.425919  8.120958e+09   \n",
      "tl  4.965548e+04  4.202574e+07  1.965166e+06   4074.083935  7.751329e+00   \n",
      "in  4.404874e+08  3.647796e+07  3.611153e+05  83196.559730  8.110808e+09   \n",
      "fr  1.904569e+09  4.641176e+07  5.835047e+06  17407.602872  8.143642e+09   \n",
      "es  7.202444e+08  4.581365e+07  3.200164e+06  18924.804286  8.140384e+09   \n",
      "\n",
      "              in           fr            es  \n",
      "nl  1.588152e+07   508.394958  1.788976e+07  \n",
      "pt  5.767966e+07  1071.377120  2.014421e+07  \n",
      "en  2.170780e+07   668.505141  3.153231e+07  \n",
      "it  8.064902e+06   552.499881  3.117114e+07  \n",
      "tl  5.972155e+07   379.942546  3.168190e+07  \n",
      "in  7.900932e+00   291.535429  1.982946e+06  \n",
      "fr  2.773087e+07     7.455647  3.148360e+07  \n",
      "es  8.438805e+06   717.415561  7.141149e+00  \n",
      "n - 3, add_one - True\n",
      "           nl         pt         en         it         tl         in  \\\n",
      "nl  11.947729  23.273540  17.672209  22.143570  22.977737  24.105975   \n",
      "pt  27.234237  10.667958  23.015003  18.935374  26.105980  28.869461   \n",
      "en  19.976047  20.993887  10.833558  19.623341  22.314976  23.764114   \n",
      "it  25.148938  17.847915  20.913164  10.855718  23.989542  27.187460   \n",
      "tl  22.727410  21.767915  17.485157  19.965150  12.226817  19.674234   \n",
      "in  21.850921  22.507749  19.459307  21.178114  18.737557  12.881496   \n",
      "fr  22.019154  19.434055  18.732888  18.707152  25.788101  25.638461   \n",
      "es  24.855799  16.552054  21.369187  17.223699  25.773830  26.600542   \n",
      "\n",
      "           fr         es  \n",
      "nl  19.843155  20.919420  \n",
      "pt  20.696080  15.787519  \n",
      "en  18.426652  18.432036  \n",
      "it  19.157588  16.001779  \n",
      "tl  22.118869  19.377478  \n",
      "in  22.258423  20.346263  \n",
      "fr  10.380005  16.681129  \n",
      "es  19.044243  10.221239  \n",
      "n - 3, add_one - False\n",
      "              nl            pt            en            it            tl  \\\n",
      "nl  4.882240e+00  5.233605e+07  5.934549e+08  9.841369e+06  1.649910e+10   \n",
      "pt  1.843639e+10  4.405599e+00  1.144237e+08  2.447058e+08  1.206840e+10   \n",
      "en  1.223968e+10  3.422140e+07  4.663513e+00  1.016820e+07  1.739865e+10   \n",
      "it  1.346062e+10  8.221309e+07  2.605410e+08  4.578528e+00  9.901893e+09   \n",
      "tl  1.986249e+09  7.093852e+07  2.564607e+08  1.175299e+08  4.755332e+00   \n",
      "in  1.289789e+10  5.154571e+07  8.120439e+09  2.012729e+08  8.485409e+09   \n",
      "fr  5.489495e+09  4.867524e+07  3.887022e+08  1.518149e+08  9.753481e+09   \n",
      "es  1.873281e+10  1.538943e+08  1.082522e+08  8.232571e+09  1.830486e+10   \n",
      "\n",
      "              in            fr            es  \n",
      "nl  7.908668e+08  7.227160e+08  1.335473e+08  \n",
      "pt  7.190969e+08  8.154551e+09  8.236058e+09  \n",
      "en  1.678180e+09  9.545158e+06  1.904795e+08  \n",
      "it  8.584588e+08  1.454759e+07  8.226562e+09  \n",
      "tl  9.020695e+08  1.743046e+08  3.041438e+08  \n",
      "in  5.176347e+00  8.109995e+09  1.218596e+08  \n",
      "fr  1.143057e+09  4.565811e+00  3.374028e+08  \n",
      "es  1.905303e+09  8.281172e+09  4.537487e+00  \n",
      "n - 4, add_one - True\n",
      "           nl         pt         en         it         tl         in  \\\n",
      "nl  21.817804  64.728242  43.957514  60.618340  61.436722  67.698555   \n",
      "pt  69.393742  19.454094  58.672516  48.847781  66.190987  76.550015   \n",
      "en  50.653913  58.537732  19.554450  54.007927  58.272179  68.073762   \n",
      "it  62.698064  44.000146  52.384060  20.000746  60.813378  71.159043   \n",
      "tl  60.264372  59.023774  42.120804  54.358780  23.274402  52.801596   \n",
      "in  58.405618  62.806431  49.652459  58.781246  46.861252  25.332942   \n",
      "fr  55.282971  51.455733  47.240518  49.794122  67.411752  71.199806   \n",
      "es  61.056207  38.707795  54.241520  42.845783  65.712664  70.569282   \n",
      "\n",
      "           fr         es  \n",
      "nl  50.029736  57.155589  \n",
      "pt  53.530019  37.710697  \n",
      "en  47.114071  52.216769  \n",
      "it  48.534839  39.311051  \n",
      "tl  58.884836  53.420801  \n",
      "in  59.650245  58.203080  \n",
      "fr  18.382299  43.474980  \n",
      "es  46.578310  18.508945  \n",
      "n - 4, add_one - False\n",
      "              nl            pt            en            it            tl  \\\n",
      "nl  2.940133e+00  4.247889e+10  1.711395e+10  6.602212e+10  1.469224e+11   \n",
      "pt  2.708072e+11  2.877103e+00  2.824801e+10  4.751248e+10  1.743791e+11   \n",
      "en  2.271742e+11  4.384030e+10  2.851359e+00  7.019289e+10  1.981254e+11   \n",
      "it  2.163911e+11  2.038379e+10  4.337187e+10  2.948341e+00  1.913100e+11   \n",
      "tl  1.774071e+11  1.583024e+10  1.950042e+10  4.220361e+10  2.958457e+00   \n",
      "in  1.553697e+11  1.608387e+10  1.836658e+10  5.607746e+10  1.103304e+11   \n",
      "fr  2.133305e+11  3.748293e+10  4.306832e+10  6.834618e+10  2.182285e+11   \n",
      "es  2.443315e+11  2.284486e+10  5.567114e+10  6.909390e+10  1.712758e+11   \n",
      "\n",
      "              in            fr            es  \n",
      "nl  1.076527e+11  4.164634e+10  5.262376e+10  \n",
      "pt  1.653166e+11  3.865455e+10  2.131075e+10  \n",
      "en  1.407748e+11  2.275699e+10  1.927740e+10  \n",
      "it  1.369229e+11  2.930908e+10  2.740574e+10  \n",
      "tl  8.413298e+10  4.006705e+10  2.789535e+10  \n",
      "in  3.221988e+00  3.482709e+10  3.233554e+10  \n",
      "fr  1.919676e+11  2.873819e+00  3.590981e+10  \n",
      "es  1.820832e+11  5.166639e+10  2.988030e+00  \n"
     ]
    }
   ],
   "source": [
    "def run_match():\n",
    "    for n in range(1, 5):\n",
    "        for add_one in [True, False]:\n",
    "            if add_one:\n",
    "                name_csv = f\"n{n}_part5\"\n",
    "            else:\n",
    "                name_csv = f\"n{n}_wo_addone_part5\"\n",
    "            perplexity_df = match(n, add_one, name_csv)\n",
    "            print(f\"n - {n}, add_one - {str(add_one)}\")\n",
    "            print(perplexity_df)\n",
    "\n",
    "run_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qD6IRIQLrlZF"
   },
   "outputs": [],
   "source": [
    "def classify(n, add_one=True):\n",
    "    with open(\"./nlp-course/lm-languages-data-new/test.json\", \"r\",\n",
    "              encoding=\"utf-8\") as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    vocabulary = preprocess()\n",
    "\n",
    "    models, files = get_files_and_models(n, vocabulary, add_one)\n",
    "\n",
    "    labels = []\n",
    "    clasification_result = []\n",
    "    score = 0\n",
    "    languages = list(models.keys())\n",
    "\n",
    "    for tweet_num, tweet_text in test_data['tweet_text'].items():\n",
    "        label = test_data[\"label\"][tweet_num]\n",
    "        labels.append(label)\n",
    "        perplexities = []\n",
    "        for lan, model in models.items():\n",
    "            perplexity = tweet_perplexity(tweet_text, n, model)\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "        guess = languages[np.argmin(perplexities)]\n",
    "        clasification_result.append(guess)\n",
    "        if label == guess:\n",
    "            score += 1\n",
    "\n",
    "    return clasification_result, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
    "\n",
    "Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF3ImVdPgAGC"
   },
   "source": [
    "```\n",
    "  model_name  f1_score\n",
    "0    Model A      0.85\n",
    "1    Model B      0.92\n",
    "2    Model C      0.87\n",
    "3    Model D      0.90\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VOBO3YQls66r"
   },
   "outputs": [],
   "source": [
    "def calc_f1():\n",
    "    perplexity_df = pd.DataFrame(columns=[\"model_name\", \"f1_score\"], index=range(6))\n",
    "    i = 0\n",
    "\n",
    "    for n in range(2, 5):\n",
    "        for add_one in [True, False]:\n",
    "            pred_list, label_list = classify(n, add_one)\n",
    "            f1 = f1_score(label_list, pred_list, average=\"macro\")\n",
    "\n",
    "            perplexity_df.loc[i][0] = f\"n={n}, add_one={add_one}\"\n",
    "            perplexity_df.loc[i][1] = f1\n",
    "            i += 1\n",
    "\n",
    "    perplexity_df.to_csv(f\"312494923_316550797_part7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "calc_f1()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfBYgfjADNPL"
   },
   "source": [
    "<br><br><br><br>\n",
    "**Part 8**  \n",
    "Let's use your Language model (dictionary) for generation (NLG).\n",
    "\n",
    "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
    "\n",
    "Some of the most commonly used methods include:\n",
    "\n",
    "> `Greedy sampling`\n",
    "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
    "\n",
    "> `Temperature scaling`  \n",
    "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
    "\n",
    "> `Top-K sampling`  \n",
    "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
    "\n",
    "> `Nucleus sampling` (also known as top-p sampling)  \n",
    "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
    "\n",
    "> `Beam search`  \n",
    "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
    "\n",
    "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
    "\n",
    "\n",
    "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbReeHtwNWKS"
   },
   "source": [
    "**Please added the needed code for each sampeling method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "v4TrLs1kI3fW"
   },
   "outputs": [],
   "source": [
    "def sample_greedy(probabilities):\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "\n",
    "def sample_temperature(probabilities, temperature=1.0, k=1):\n",
    "    logits = np.log(probabilities) / temperature\n",
    "    probs = softmax(logits)\n",
    "    samples = np.random.choice(len(probabilities), size=k, p=probs)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def sample_topK(probabilities, k=1):\n",
    "    sorted_indx = np.argsort(probabilities)[::-1][:k]\n",
    "    return sorted_indx\n",
    "\n",
    "\n",
    "def sample_topP(probabilities, p=0.9):\n",
    "    sum = 0\n",
    "    sorted_indexs = np.argsort(probabilities)[::-1]\n",
    "    for i, p_arg in enumerate(sorted_indexs):\n",
    "        sum += probabilities[p_arg]\n",
    "        if sum >= p:\n",
    "            break\n",
    "    return random.choices(sorted_indexs[:i + 1], weights=np.array(probabilities)[sorted_indexs[:i + 1]], k=1)[0]\n",
    "\n",
    "\n",
    "def sample_beam(model, gen_length, start_tokens, stop_token, vocabulary, k):\n",
    "    max_val = 0\n",
    "    char_max_val = None\n",
    "\n",
    "    if start_tokens[-len(stop_token):] == stop_token or gen_length == 0:\n",
    "        return 1, stop_token\n",
    "\n",
    "    if start_tokens not in model:\n",
    "        choosen_chars = np.random.choice(vocabulary, size=k)\n",
    "    else:\n",
    "        root = model[start_tokens]\n",
    "        chars = list(root.keys())\n",
    "        probabilities = list(root.values())\n",
    "        choosen_chars = np.array(chars)[sample_topK(probabilities, k=k)]\n",
    "\n",
    "    for char in choosen_chars:\n",
    "        if start_tokens not in model:\n",
    "            val, _ = sample_beam(model, gen_length - 1, start_tokens[1:] + char, stop_token,\n",
    "                                 vocabulary, k)\n",
    "            prob = model[\"not_exist\"] * val\n",
    "        else:\n",
    "            if char not in model[start_tokens]:\n",
    "                val, _ = sample_beam(model, gen_length - 1, start_tokens[1:] + char,\n",
    "                                     stop_token, vocabulary, k)\n",
    "                prob = root[\"not_exist\"] * val\n",
    "            else:\n",
    "                val, _ = sample_beam(model, gen_length - 1, start_tokens[1:] + char, stop_token, vocabulary, k)\n",
    "                prob = root[char] * val\n",
    "        if prob > max_val:\n",
    "            max_val = prob\n",
    "            char_max_val = char\n",
    "\n",
    "    return max_val, char_max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Giylo6-lI21t"
   },
   "source": [
    "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
    "Notice the 4 core issues: \n",
    "- Starting tokens\n",
    "- Length of the generation\n",
    "- Sampling methond (use all)\n",
    "- Stop Token (if this token is sampled, stop generating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9pUOkRtjN1mI"
   },
   "outputs": [],
   "source": [
    "test_ = {\n",
    "    'example1' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['greedy','beam'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example2' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['temperature','topK','topP'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example3' : {\n",
    "        'start_tokens' : \"He\",\n",
    "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
    "        'gen_length' : \"20\",\n",
    "        'stop_token' : \"me\",\n",
    "        'generation' : []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTbF-9zKVchQ"
   },
   "source": [
    "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x) / np.sum(np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3zf-omUXQezz"
   },
   "outputs": [],
   "source": [
    "vocabulary = preprocess()\n",
    "path_json = \"./nlp-course/lm-languages-data-new/en.json\"\n",
    "\n",
    "for k, v in test_.items():\n",
    "    start_tokens = v[\"start_tokens\"]\n",
    "    stop_token = v[\"stop_token\"]\n",
    "    n = len(start_tokens) + 1\n",
    "    model = lm(n, vocabulary, path_json, True)\n",
    "    for sm in v[\"sampling_method\"]:\n",
    "        generation_result = v[\"start_tokens\"]\n",
    "        gen_length = int(v[\"gen_length\"])# - len(v[\"start_tokens\"])\n",
    "\n",
    "        while gen_length != 0:\n",
    "            start_tokens = generation_result[-(n - 1):]\n",
    "\n",
    "            next_char = None\n",
    "            if sm != \"beam\":\n",
    "                if start_tokens not in model:\n",
    "                    next_char = np.random.choice(vocabulary, size=1)[0]\n",
    "                else:\n",
    "                    root = model[start_tokens]\n",
    "                    probabilities = []\n",
    "                    for c in vocabulary:\n",
    "                        if c in root:\n",
    "                            probabilities.append(root[c])\n",
    "                        else:\n",
    "                            probabilities.append(root[\"not_exist\"])\n",
    "\n",
    "            if sm == \"greedy\":\n",
    "                next_char = vocabulary[sample_greedy(probabilities)]\n",
    "            elif sm == \"beam\":\n",
    "                _, next_char = sample_beam(model, gen_length, start_tokens, stop_token=stop_token,\n",
    "                                           vocabulary=vocabulary, k=2)\n",
    "            elif sm == \"temperature\":\n",
    "                smapels = np.array(vocabulary)[sample_temperature(probabilities, temperature=0.5, k=2)]\n",
    "                next_char = np.random.choice(smapels, size=1)[0]\n",
    "            elif sm == \"topK\":\n",
    "                smapels = sample_topK(probabilities, k=3)\n",
    "                next_char = \\\n",
    "                random.choices(np.array(vocabulary)[smapels], weights=np.array(probabilities)[smapels], k=1)[0]\n",
    "            elif sm == \"topP\":\n",
    "                next_char = vocabulary[sample_topP(probabilities, p=0.2)]\n",
    "\n",
    "            generation_result = generation_result + next_char\n",
    "            if generation_result[-len(stop_token):] == stop_token:\n",
    "                break\n",
    "            gen_length -= 1\n",
    "        v[\"generation\"].append(generation_result[(n-1):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bvla30-lVw8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- NLG --------\n",
      "example1:\n",
      "\tgreedy >> Hon t t t t\n",
      "\tbeam >> Hous://///t\n",
      "\n",
      "example2:\n",
      "\ttemperature >> Hored T her\n",
      "\ttopK >> Hout aren t\n",
      "\ttopP >> He the are \n",
      "\n",
      "example3:\n",
      "\tgreedy >> Heall the the the the \n",
      "\tbeam >> Heally https://t.come\n",
      "\ttemperature >> Heal lon a se ou anne \n",
      "\ttopK >> He will aliked ther a \n",
      "\ttopP >> He'<e>r the the the the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### do not change ###\n",
    "print('-------- NLG --------')\n",
    "\n",
    "for k,v in test_.items():\n",
    "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
    "  print(f'{k}:')\n",
    "  print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br><br><br>\n",
    "# **Good luck!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}