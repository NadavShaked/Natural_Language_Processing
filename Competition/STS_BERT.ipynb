{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Link to compentition\n",
        "https://www.kaggle.com/competitions/runi-nlp-2023/leaderboard\n",
        "\n",
        "#### Useful links\n",
        "https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=similar"
      ],
      "metadata": {
        "id": "q_x-zyR6CrSS"
      },
      "id": "q_x-zyR6CrSS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN NLP 2023 course competition\n",
        "#### Overview:\n",
        "Welcome to the RUN NLP 2023 course competition!\n",
        "This competition is designed to help you learn how to apply natural language processing (NLP) techniques and language models (LM) to validate how similar a pair of texts are. We invite you to compete and challenge yourself.\n",
        "\n",
        "#### Goal:\n",
        "Text Similarity is the process of comparing a piece of text with another and finding the similarity between them. It’s basically about determining the degree of closeness of the text. Text Similarity can be used for a variety of purposes, including search engines, summarization, essay scoring, plagiarism detection, machine translation, and more. Text similarity algorithms can vary greatly depending on the type of text and the purpose it is being used for.\n",
        "Generally, text similarity algorithms will compare two texts by looking at the words and phrases used in each of the texts, looking at the phrase or sentence structure, or examining the differences between the two documents in terms of context. The algorithms may also consider other factors, such as the length of the texts, the number of words, or the number of sentences. Additionally, the NLP algorithms use semantic information to determine the degree of similarity between two texts. By using a text similarity algorithm, users can quickly and accurately determine the degree of similarity between two texts.\n",
        "And this is what you'll try to achieve in this competition :)\n",
        "\n",
        "#### Instructions:\n",
        "\n",
        "Register for the competition by clicking the Register button at the top of the page.\n",
        "Download the data and read the task description.\n",
        "Develop your NLP LM model.\n",
        "Submit your answer.\n",
        "Track the progress of your submission on the Leaderboard.\n",
        "After the competition ends, participants with the top scores will be rewarded with certificates of recognition.\n",
        "We wish you the best of luck in this competition!\n",
        "\n",
        "~~~~\n",
        "\n",
        "#### Acknowledgements\n",
        "Special thanks to Dr. Kfir Bar, Mr. Amir Cohen, and Mr. Sahar Millis"
      ],
      "metadata": {
        "id": "xl42jWb3CzwT"
      },
      "id": "xl42jWb3CzwT"
    },
    {
      "cell_type": "markdown",
      "id": "28df765c-ec6b-450b-a8b4-b40c65f73159",
      "metadata": {
        "id": "28df765c-ec6b-450b-a8b4-b40c65f73159"
      },
      "source": [
        "# Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bc873328-78e8-4e81-bb0b-fc1ba41bcb82",
      "metadata": {
        "tags": [],
        "id": "bc873328-78e8-4e81-bb0b-fc1ba41bcb82"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install datasets\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002b8bb8-e806-48a8-ab70-8c8f35d13466",
      "metadata": {
        "id": "002b8bb8-e806-48a8-ab70-8c8f35d13466"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d8fe01b5-6a9a-458d-a97f-1fb20b621b0f",
      "metadata": {
        "id": "d8fe01b5-6a9a-458d-a97f-1fb20b621b0f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv_file_for_train(file_path):\n",
        "    data_frame = pd.read_csv(file_path)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for i in range(len(data_frame)):\n",
        "        data.append({\n",
        "            \"sentence1\": data_frame[\"text1\"][i],\n",
        "            \"sentence2\": data_frame[\"text2\"][i],\n",
        "            \"similarity_score\": float(data_frame[\"Similarity\"][i]),\n",
        "            \"similarity\": float(data_frame[\"Similarity\"][i])\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "def read_csv_file_for_test(test_sentences_file_path, test_sample_label_file_path):\n",
        "    test_sentences_data_frame = pd.read_csv(test_sentences_file_path)\n",
        "    test_sample_label_data_frame = pd.read_csv(test_sample_label_file_path)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for i in range(len(test_sentences_data_frame)):\n",
        "        data.append({\n",
        "            \"sentence1\": test_sentences_data_frame[\"text1\"][i],\n",
        "            \"sentence2\": test_sentences_data_frame[\"text2\"][i],\n",
        "            \"similarity_score\": float(test_sample_label_data_frame[\"Category\"][i]),\n",
        "            \"similarity\": float(test_sample_label_data_frame[\"Category\"][i])\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "tarin = read_csv_file_for_train(\"./nlp_2023_train.csv\")\n",
        "test = read_csv_file_for_test(\"./nlp_2023_test.csv\", \"./nlp_2023_sample.csv\")"
      ],
      "metadata": {
        "id": "GX7m_28el6aX"
      },
      "id": "GX7m_28el6aX",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0bd9f3e6-9625-4c64-bb76-5e90911239a6",
      "metadata": {
        "id": "0bd9f3e6-9625-4c64-bb76-5e90911239a6"
      },
      "source": [
        "# Fetch data for training and test, as well as the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0fc07b38-8951-4d47-a5e1-ecdf810cdd93",
      "metadata": {
        "id": "0fc07b38-8951-4d47-a5e1-ecdf810cdd93"
      },
      "outputs": [],
      "source": [
        "# Dataset for training\n",
        "dataset = tarin\n",
        "\n",
        "similarity = [i['similarity_score'] for i in dataset]\n",
        "normalized_similarity = [i/1.0 for i in similarity]\n",
        "\n",
        "# Dataset for test\n",
        "test_dataset = test\n",
        "\n",
        "# Prepare test data\n",
        "sentence_1_test = [i['sentence1'] for i in test_dataset]\n",
        "sentence_2_test = [i['sentence2'] for i in test_dataset]\n",
        "text_cat_test = [[str(x), str(y)] for x,y in zip(sentence_1_test, sentence_2_test)]\n",
        "\n",
        "# Set the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5299ca8-d00d-4151-8390-86c97403785d",
      "metadata": {
        "id": "f5299ca8-d00d-4151-8390-86c97403785d"
      },
      "source": [
        "# Define Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8f44b7c4-d15c-4b79-bc5e-371d5bd42ffe",
      "metadata": {
        "id": "8f44b7c4-d15c-4b79-bc5e-371d5bd42ffe"
      },
      "outputs": [],
      "source": [
        "class STSBertModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(STSBertModel, self).__init__()\n",
        "\n",
        "        word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=128)\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        self.sts_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "        parameters = self.sts_model.parameters()\n",
        "        # Determine which layers to freeze\n",
        "        for idx, param in enumerate(parameters):\n",
        "            if idx < 196:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "              break\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = self.sts_model(input_data)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3103d5bd-e4f3-4b98-a1c6-cfc354f9edca",
      "metadata": {
        "id": "3103d5bd-e4f3-4b98-a1c6-cfc354f9edca"
      },
      "source": [
        "# Define Dataloader for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "98ae2242-af8f-4c76-a21a-4baeeed3ae43",
      "metadata": {
        "id": "98ae2242-af8f-4c76-a21a-4baeeed3ae43"
      },
      "outputs": [],
      "source": [
        "class DataSequence(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        similarity = [i['similarity_score'] for i in dataset]\n",
        "        self.label = [i/1.0 for i in similarity]\n",
        "        self.sentence_1 = [i['sentence1'] for i in dataset]\n",
        "        self.sentence_2 = [i['sentence2'] for i in dataset]\n",
        "        self.text_cat = [[str(x), str(y)] for x,y in zip(self.sentence_1, self.sentence_2)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_cat)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return torch.tensor(self.label[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        return tokenizer(self.text_cat[idx], padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "def collate_fn(texts):\n",
        "  num_texts = len(texts['input_ids'])\n",
        "  features = list()\n",
        "  for i in range(num_texts):\n",
        "      features.append({'input_ids':texts['input_ids'][i], 'attention_mask':texts['attention_mask'][i]})\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd81992-4a05-4f48-9869-83b38b7a3f90",
      "metadata": {
        "id": "3fd81992-4a05-4f48-9869-83b38b7a3f90"
      },
      "source": [
        "# Define loss function for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "78ca0312-8648-4101-9429-7286d6268bb5",
      "metadata": {
        "id": "78ca0312-8648-4101-9429-7286d6268bb5"
      },
      "outputs": [],
      "source": [
        "class CosineSimilarityLoss(torch.nn.Module):\n",
        "    def __init__(self,  loss_fct = torch.nn.MSELoss(), cos_score_transformation=torch.nn.Identity()):\n",
        "        super(CosineSimilarityLoss, self).__init__()\n",
        "        self.loss_fct = loss_fct\n",
        "        self.cos_score_transformation = cos_score_transformation\n",
        "        self.cos = torch.nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        embedding_1 = torch.stack([inp[0] for inp in input])\n",
        "        embedding_2 = torch.stack([inp[1] for inp in input])\n",
        "\n",
        "        output = self.cos_score_transformation(self.cos(embedding_1, embedding_2))\n",
        "\n",
        "        return self.loss_fct(output, label.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc17726-80e4-409d-8183-1ec17d2e05da",
      "metadata": {
        "id": "0fc17726-80e4-409d-8183-1ec17d2e05da"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bcf41a99-3ce1-4125-b464-d3b8d0d295af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcf41a99-3ce1-4125-b464-d3b8d0d295af",
        "outputId": "722e96bc-72d6-432e-a0ec-b32a9a4a32ec"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 1094/1094 [21:34<00:00,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Loss:  0.004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def model_train(dataset, epochs, learning_rate, bs):\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    model = STSBertModel()\n",
        "\n",
        "    criterion = CosineSimilarityLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_dataset = DataSequence(dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=bs, shuffle=True)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for i in range(epochs):\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0.0\n",
        "\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "            train_data['input_ids'] = train_data['input_ids'].to(device)\n",
        "            train_data['attention_mask'] = train_data['attention_mask'].to(device)\n",
        "            del train_data['token_type_ids']\n",
        "\n",
        "            train_data = collate_fn(train_data)\n",
        "\n",
        "            output = [model(feature)['sentence_embedding'] for feature in train_data]\n",
        "\n",
        "            loss = criterion(output, train_label.to(device))\n",
        "            total_loss_train += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f'Epochs: {i + 1} | Loss: {total_loss_train / len(dataset): .3f}')\n",
        "        model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Train the model\n",
        "trained_model = model_train(dataset, EPOCHS, LEARNING_RATE, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f8052fa6-8a87-4378-87c1-1714f55a88bc",
      "metadata": {
        "id": "f8052fa6-8a87-4378-87c1-1714f55a88bc"
      },
      "outputs": [],
      "source": [
        "# Function to predict test data\n",
        "def predict_sts(texts):\n",
        "  trained_model.to('cpu')\n",
        "  trained_model.eval()\n",
        "  test_input = tokenizer(texts, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
        "  test_input['input_ids'] = test_input['input_ids']\n",
        "  test_input['attention_mask'] = test_input['attention_mask']\n",
        "  del test_input['token_type_ids']\n",
        "\n",
        "  test_output = trained_model(test_input)['sentence_embedding']\n",
        "  sim = torch.nn.functional.cosine_similarity(test_output[0], test_output[1], dim=0).item()\n",
        "\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f70635-7047-46b5-b313-e2e6963ffdab",
      "metadata": {
        "id": "d4f70635-7047-46b5-b313-e2e6963ffdab"
      },
      "source": [
        "# Predict on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e4fde3ae-db23-4135-aa04-4a79d040b089",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4fde3ae-db23-4135-aa04-4a79d040b089",
        "outputId": "929dc400-dd66-4426-8dfe-8d50e0d47b4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.659848153591156"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "predict_sts(text_cat_test[245])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f757ad3e-3ea6-4706-a4f3-4a090fa6dbd2",
      "metadata": {
        "id": "f757ad3e-3ea6-4706-a4f3-4a090fa6dbd2"
      },
      "outputs": [],
      "source": [
        "predict_sts(text_cat_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}